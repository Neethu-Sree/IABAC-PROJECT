{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a840627",
   "metadata": {},
   "source": [
    "##### Importing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074730bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98d97c",
   "metadata": {},
   "source": [
    "##### Loading  the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5630cd0",
   "metadata": {},
   "source": [
    "###### Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n",
    "data.tail()\n",
    "data.shape\n",
    "\n",
    "data.dtypes\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data['PerformanceRating'].value_counts()\n",
    "\n",
    "data['PerformanceRating'].unique()\n",
    "\n",
    "data=data.drop(['EmpNumber'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52809a2b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Univariate Analysis\n",
    "\n",
    "!pip install sweetviz     \n",
    "\n",
    "import sweetviz as sv            # library for univariant analysis\n",
    "my_report = sv.analyze(data)     # pass the original dataframe\n",
    "my_report.show_html() \n",
    "\n",
    "\n",
    "## Bivariate Analysis\n",
    "\n",
    "## Create a new dataframe with numerical variables only(Check the datatype by using info function)\n",
    "data2=data[['Age','DistanceFromHome','EmpEducationLevel','EmpEnvironmentSatisfaction','EmpHourlyRate','EmpJobInvolvement','EmpJobLevel','EmpJobSatisfaction','NumCompaniesWorked','EmpLastSalaryHikePercent','EmpRelationshipSatisfaction','TotalWorkExperienceInYears', 'TrainingTimesLastYear',\n",
    "       'EmpWorkLifeBalance', 'ExperienceYearsAtThisCompany','ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']]\n",
    "\n",
    "plt.figure(figsize=(35,25), facecolor='white')#To set canvas \n",
    "plotnumber = 1#counter\n",
    "\n",
    "for column in data2:#accessing the columns \n",
    "    if plotnumber<=28:\n",
    "        ax = plt.subplot(7,4,plotnumber)\n",
    "        sns.countplot(x=data2[column]\n",
    "                        ,hue=data.PerformanceRating)\n",
    "        plt.xlabel(column,fontsize=20)#assign name to x-axis and set font-20\n",
    "        plt.ylabel('PerformanceRating',fontsize=20)\n",
    "    plotnumber+=1#counter increment\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "## Create a new dataframe with categorical variables only(Check the datatype by using info function)\n",
    "data1=data[['Gender','EducationBackground','MaritalStatus','EmpDepartment','EmpJobRole','BusinessTravelFrequency','OverTime','Attrition']]\n",
    "\n",
    "# Plotting how every  categorical feature correlate with the \"target\"\n",
    "plt.figure(figsize=(50,50), facecolor='white')#canvas size\n",
    "plotnumber = 1#count variable\n",
    "\n",
    "for column in data1:#for loop to acess columns form data1\n",
    "    if plotnumber<=16:#checking whether count variable is less than 16 or not\n",
    "        ax = plt.subplot(4,4,plotnumber)#plotting 8 graphs in canvas(4 rows and 4 columns)\n",
    "        sns.countplot(x=data1[column].dropna(axis=0)#plotting count plot \n",
    "                        ,hue=data.PerformanceRating)\n",
    "        plt.xlabel(column,fontsize=20)#assigning name to x-axis and increasing it's font \n",
    "        plt.ylabel('PerformanceRating',fontsize=20)#assigning name to y-axis and increasing it's font \n",
    "    plotnumber+=1#increasing counter\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "## Analysis of Department wise performance for each department seperately\n",
    "\n",
    "# A new pandas Dataframe is created to analyze department wise performance\n",
    "dept = data.iloc[:,[4,26]].copy()\n",
    "dept_per = dept.copy()\n",
    "\n",
    "# Creating a new dataframe to analyze each department separately\n",
    "department = pd.get_dummies(dept_per['EmpDepartment'])\n",
    "performance = pd.DataFrame(dept_per['PerformanceRating'])\n",
    "dept_rating = pd.concat([department,performance],axis=1)\n",
    "\n",
    "plt.figure(figsize=(20,10),facecolor='white')\n",
    "ax = sns.countplot(x='EmpDepartment',hue=data.PerformanceRating,data=data)\n",
    "for i in ax.patches:\n",
    "    ax.annotate('{:.0f}'.format(i.get_height()), (i.get_x()+0.06, i.get_height()+2))\n",
    "plt.title('Employee Departments vs PerformanceRating',fontsize=20)\n",
    "plt.xlabel('EmpDepartment',fontsize=20)\n",
    "plt.ylabel('PerformanceRating',fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ea8df",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = []#list\n",
    "for column in data.columns:#for loop to acess columns form dataset\n",
    "    if data[column].dtype == object and len(data[column].unique()) <= 50:#checking datatype whether datatype is object/string and number of unique label in the columns less than 50 \n",
    "        categorical_col.append(column)#appending those columns in the list who statisfy the condition \n",
    "        print(f\"{column} : {data[column].unique()}\")#output\n",
    "        print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3531e",
   "metadata": {},
   "source": [
    "#### Converting categorical variables into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Gender.value_counts()\n",
    "\n",
    "data.Gender=pd.get_dummies(data.Gender,drop_first=True)\n",
    "data.Gender\n",
    "\n",
    "\n",
    "data.EducationBackground=data.EducationBackground.map({'Life Sciences':5,'Medical':4,'Marketing':3,'Technical Degree':2,'Other':1,'Human Resources':0 })\n",
    "\n",
    "\n",
    "data.EmpDepartment=data.EmpDepartment.map({'Research & Development':3,'Sales':5,'Human Resources':2,'Development':4,'Finance':1,'Data Science':0})#imputation using map function\n",
    "\n",
    "\n",
    "data.BusinessTravelFrequency=data.BusinessTravelFrequency.map({'Travel_Frequently':1,'Travel_Rarely':2,'Non-Travel':0})\n",
    "\n",
    "\n",
    "data.Attrition=data.Attrition.map({'Yes':1,'No':0})\n",
    "\n",
    "data.EmpJobRole=data.EmpJobRole.map({'Sales Executive':18, 'Manager':11, 'Developer':17, 'Sales Representative':14,\n",
    " 'Human Resources':9, 'Senior Developer':12, 'Data Scientist':5,\n",
    " 'Senior Manager R&D':1, 'Laboratory Technician':13, 'Manufacturing Director':7,\n",
    " 'Research Scientist':15, 'Healthcare Representative':6, 'Research Director':4,\n",
    " 'Manager R&D':16, 'Finance Manager':10, 'Technical Architect':1, 'Business Analyst':3,\n",
    " 'Technical Lead':8, 'Delivery Manager':0})\n",
    "\n",
    "## Encoding MaritalStatus\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder#importing label encoder from sklearn \n",
    "\n",
    "label = LabelEncoder()#object creation \n",
    "data.MaritalStatus=label.fit_transform(data.MaritalStatus)#applying label encoder to  marital status\n",
    "\n",
    "data.OverTime=label.fit_transform(data.OverTime)#label encoding\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11852a",
   "metadata": {},
   "source": [
    "### Distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce85055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1 #maintian count for graph\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=28 :# as there are 20 columns in the data\n",
    "        ax = plt.subplot(7,4,plotnumber)# plotting 20 graphs (5-rows,4-columns) ,plotnumber is for count \n",
    "        sns.distplot(data[column])#plotting dist plot to know distribution\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03964f4c",
   "metadata": {},
   "source": [
    "### Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,35), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=28:\n",
    "        ax = plt.subplot(7,4,plotnumber)\n",
    "        sns.boxplot(data[column]) \n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "IQR=stats.iqr(data.TrainingTimesLastYear,interpolation='midpoint')\n",
    "IQR\n",
    "\n",
    "Q1=data.TrainingTimesLastYear.quantile(0.25)\n",
    "Q3=data.TrainingTimesLastYear.quantile(0.75)\n",
    "min_limit=Q1-1.5*IQR\n",
    "max_limit=Q3+1.5*IQR\n",
    "max_limit\n",
    "\n",
    "data.loc[data['TrainingTimesLastYear'] > max_limit,'TrainingTimesLastYear']=data['TrainingTimesLastYear'].median()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(data=data[['TrainingTimesLastYear']])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6044af1",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "## Checking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79289770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))#canvas size\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15}) # plotting heat map to check correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a28300",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa211cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating independent and dependent variable\n",
    "X = data.drop('PerformanceRating', axis=1)#independent variable \n",
    "y = data.PerformanceRating#dependent variable \n",
    "\n",
    "## creating training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,random_state=3)\n",
    "\n",
    "# Checking the dimension of our train and test splits\n",
    "print('Shape of x_train: ',X_train.shape)\n",
    "print('Shape of y_train: ',y_train.shape)\n",
    "print('Shape of x_test: ',X_test.shape)\n",
    "print('Shape of y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a74c74",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063511f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier #importing decision tree from sklearn.tree\n",
    "dt=DecisionTreeClassifier() #object creation for decision tree  \n",
    "dt.fit(X_train, y_train) #training the model\n",
    "\n",
    "y_hat=dt.predict(X_train)#prediction\n",
    "y_hat1=dt.predict(X_test)#prediction\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,f1_score,recall_score,precision_score\n",
    "\n",
    "print(classification_report(y_train,y_hat)) # train data\n",
    "\n",
    "print(classification_report(y_test,y_hat1)) # test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae8abf",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#creating dictionary--> key value pair of hyperparameters having key as parameter and values as its values\n",
    "params = {\n",
    "    \"criterion\":(\"gini\", \"entropy\"), #quality of split\n",
    "    \"splitter\":(\"best\", \"random\"), # searches the features for a split\n",
    "    \"max_depth\":(list(range(1,20))), #depth of tree range from 1 to 19\n",
    "    \"min_samples_split\":[2, 3, 4],    #the minimum number of samples required to split internal node\n",
    "    \"min_samples_leaf\":list(range(1, 20)),#minimum number of samples required to be at a leaf node,we are passing list which is range from 1 to 19 \n",
    "}\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=3)#object creation for decision tree with random state 3\n",
    "tree_cv = GridSearchCV(tree_clf, params, scoring=\"f1\", n_jobs=-1, verbose=3, cv=3)\n",
    "\n",
    "#passing model to gridsearchCV \n",
    "\n",
    "#tree_cv.fit(X_train,y_train)#training data on gridsearch cv\n",
    "#best_params = tree_cv.best_params_#it will give you best parameters \n",
    "#print(f\"Best paramters: {best_params})\")#printing  best parameters\n",
    "\n",
    "dt1 = DecisionTreeClassifier(criterion='gini',max_depth=6,min_samples_leaf= 1,min_samples_split=2,splitter='best')#passing best parameter to decision tree\n",
    "\n",
    "dt1.fit(X_train, y_train)#traing model with best parameter\n",
    "\n",
    "y_pred_train1  = dt1.predict(X_train)\n",
    "y_hat1 = dt1.predict(X_test)#predicting\n",
    "\n",
    "print(classification_report(y_train, y_pred_train1))   #train data\n",
    "\n",
    "print(classification_report(y_test,y_hat1))#it will give precision,recall,f1 scores and accuracy #test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335be9e3",
   "metadata": {},
   "source": [
    "## 2. RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485536a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "MR=RandomForestClassifier(n_estimators=100)\n",
    "MR.fit(X_train,y_train)\n",
    "\n",
    "rf_hat=MR.predict(X_train)#prediction\n",
    "rf_pred=MR.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train,rf_hat))\n",
    "\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5801274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier #importing decision tree from sklearn.tree\n",
    "dt=DecisionTreeClassifier() #object creation for decision tree  \n",
    "dt.fit(X_train, y_train) #training the model\n",
    "\n",
    "y_hat=dt.predict(X_train)#prediction\n",
    "y_hat1=dt.predict(X_test)#prediction\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,f1_score,recall_score,precision_score\n",
    "\n",
    "print(classification_report(y_train,y_hat)) # train data\n",
    "\n",
    "print(classification_report(y_test,y_hat1)) # test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f3a4a",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]#List Comprehension-using for loop in list\n",
    "max_features = ['auto', 'sqrt']#maximum number of features allowed to try in individual tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num=10)]#List Comprehension-using for loop in list\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]#minimum number of samples required to split an internal node\n",
    "min_samples_leaf = [1, 2, 4]#minimum number of samples required to be at a leaf node.\n",
    "bootstrap = [True, False]#sampling \n",
    "\n",
    "#dictionary for hyperparameters\n",
    "random_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n",
    "               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n",
    "\n",
    "rf_clf1 = RandomForestClassifier(random_state=42)#model\n",
    "\n",
    "rf_cv = RandomizedSearchCV(estimator=rf_clf1,param_distributions=random_grid, n_iter=100, cv=3, \n",
    "                               verbose=2, random_state=42, n_jobs=-1)\n",
    "#estimator--number of decision tree\n",
    "#scoring--->performance matrix to check performance\n",
    "#param_distribution-->hyperparametes(dictionary we created)\n",
    "#n_iter--->Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.default=10\n",
    "##cv------> number of flods\n",
    "#verbose=Controls the verbosity: the higher, the more messages.\n",
    "#n_jobs---->Number of jobs to run in parallel,-1 means using all processors.\n",
    "\n",
    "#rf_cv.fit(X_train, y_train)##training data on randomsearch cv\n",
    "#rf_best_params = rf_cv.best_params_##it will give you best parameters \n",
    "#print(f\"Best paramters: {rf_best_params})\")##printing  best parameters\n",
    "\n",
    "rf_clf2 = RandomForestClassifier(n_estimators=400,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',max_depth=30,bootstrap=True)#passing best parameter to randomforest\n",
    "rf_clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train  = rf_clf2.predict(X_train)#training\n",
    "y_predict=rf_clf2.predict(X_test)#testing\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))   #train data\n",
    "\n",
    "print(classification_report(y_test, y_predict))   #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67440e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=rf_clf2.feature_importances_\n",
    "imp\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(y=data.columns[:26],x=imp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a42c3",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "dt=GradientBoostingClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred2=dt.predict(X_train)\n",
    "y_pred3=dt.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train,y_pred2))   #train data\n",
    "\n",
    "print(classification_report(y_test, y_pred3)) #test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a7a34",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783db1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb_classifier, param_grid, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#best_params = grid_search.best_params_\n",
    "#print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "best_gb_classifier = GradientBoostingClassifier(learning_rate= 0.01, max_depth=3, n_estimators= 300)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "best_gb_classifier.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_ht=best_gb_classifier.predict(X_train)\n",
    "y_hat3_pr = best_gb_classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train,y_ht))\n",
    "\n",
    "print(classification_report(y_test,y_hat3_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fa63f",
   "metadata": {},
   "source": [
    "## 4. Artificial Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100),batch_size=10,learning_rate_init=0.01,max_iter=2000,random_state=10)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_hat_mlp = mlp.predict(X_train)\n",
    "y_predict_mlp = mlp.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train,y_hat_mlp))\n",
    "\n",
    "print(classification_report(y_test,y_predict_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ce0bd",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b320ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(scaled_data, y,random_state=3)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = LR.predict(X_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train, y_pred1)) #train data\n",
    "\n",
    "print(classification_report(y_test, y_pred)) #test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46db79a",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC() ## base model with default pbarameters\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "sv_hat=svclassifier.predict(X_train)#prediction\n",
    "y_hat=svclassifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train,sv_hat))\n",
    "\n",
    "print(classification_report(y_test,y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2359de",
   "metadata": {},
   "source": [
    "## Bagging for Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier#import bagging \n",
    "model_bagg1=BaggingClassifier(base_estimator=svclassifier,n_estimators=20) ## model objet creation\n",
    "#base_estimator---> algorithm which you want to pass\n",
    "#n_estimotors-----> number of base learners\n",
    "model_bagg1.fit(X_train,y_train) ## fitting the model\n",
    "\n",
    "y_hat_bagg=model_bagg1.predict(X_test) ## getting the prediction\n",
    "y_hat_bagg1=model_bagg1.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train,y_hat_bagg1))\n",
    "\n",
    "print(classification_report(y_test,y_hat_bagg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f97666",
   "metadata": {},
   "source": [
    "## 7. K Nearest Neighbour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier#USING KNN\n",
    "KNN1=KNeighborsClassifier(n_neighbors=5) ## model object creation\n",
    "KNN1.fit(X_train,y_train)  ## fitting the model\n",
    "\n",
    "y_hat_knn=KNN1.predict(X_test) ## getting the predict from created model\n",
    "knn_tr=KNN1.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train,knn_tr))\n",
    "\n",
    "print(classification_report(y_test,y_hat_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9420c998",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba548de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=scaled_data,columns=X.columns)\n",
    "\n",
    "## Getting the optimal number of PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "principalComponents=pca.fit_transform(data)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\" Number of Components\")\n",
    "plt.ylabel('Variance (%)') # for each components\n",
    "plt.title(\"Explained Variance\")\n",
    "plt.show()\n",
    "\n",
    "pca=PCA(n_components=10)\n",
    "new_data=pca.fit_transform(df)\n",
    "i=1\n",
    "for i in range(1,10):\n",
    "    i=i+1\n",
    "    print(i)\n",
    "principal_df=pd.DataFrame(data=new_data,columns=print(i))\n",
    "\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a7327",
   "metadata": {},
   "source": [
    "\n",
    "## Fitting Models after Principal Component Analysis\n",
    "\n",
    "# Logistic Regression with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data=scaler.fit_transform(new_data)\n",
    "\n",
    "X_train_pca,X_test_pca,y_train,y_test=train_test_split(scaled_data, y,random_state=3)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred1 = LR.predict(X_train_pca)\n",
    "y_pred = LR.predict(X_test_pca)\n",
    "\n",
    "print(classification_report(y_test, y_pred)) #test data\n",
    "\n",
    "print(classification_report(y_train, y_pred1)) #train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f622d4c",
   "metadata": {},
   "source": [
    "## RandomForest Classifier with Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89285ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "MR=RandomForestClassifier(n_estimators=100)\n",
    "MR.fit(X_train_pca,y_train)\n",
    "\n",
    "rf_hat=MR.predict(X_train_pca)#prediction\n",
    "rf_pred=MR.predict(X_test_pca)\n",
    "\n",
    "print(classification_report(y_train,rf_hat))\n",
    "\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86576af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tunning for RandomForest with Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c37bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]#List Comprehension-using for loop in list\n",
    "max_features = ['auto', 'sqrt']#maximum number of features allowed to try in individual tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num=10)]#List Comprehension-using for loop in list\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]#minimum number of samples required to split an internal node\n",
    "min_samples_leaf = [1, 2, 4]#minimum number of samples required to be at a leaf node.\n",
    "bootstrap = [True, False]#sampling \n",
    "\n",
    "#dictionary for hyperparameters\n",
    "random_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n",
    "               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n",
    "\n",
    "rf_clf1 = RandomForestClassifier(random_state=42)#model\n",
    "\n",
    "rf_cv = RandomizedSearchCV(estimator=rf_clf1,param_distributions=random_grid, n_iter=100, cv=3, \n",
    "                               verbose=2, random_state=42, n_jobs=-1)\n",
    "#estimator--number of decision tree\n",
    "#scoring--->performance matrix to check performance\n",
    "#param_distribution-->hyperparametes(dictionary we created)\n",
    "#n_iter--->Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.default=10\n",
    "##cv------> number of flods\n",
    "#verbose=Controls the verbosity: the higher, the more messages.\n",
    "#n_jobs---->Number of jobs to run in parallel,-1 means using all processors.\n",
    "\n",
    "#rf_cv.fit(X_train, y_train)##training data on randomsearch cv\n",
    "#rf_best_params = rf_cv.best_params_##it will give you best parameters \n",
    "#print(f\"Best paramters: {rf_best_params})\")##printing  best parameters\n",
    "\n",
    "rf_clf2 = RandomForestClassifier(n_estimators=400,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',max_depth=30,bootstrap=True)#passing best parameter to randomforest\n",
    "rf_clf2.fit(X_train_pca, y_train)\n",
    "\n",
    "y_predict=rf_clf2.predict(X_test_pca)#testing\n",
    "y_pred_train  = rf_clf2.predict(X_train_pca)#training\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))   \n",
    "\n",
    "print(classification_report(y_test, y_predict))   #test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
