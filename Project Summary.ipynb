{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8641b526",
   "metadata": {},
   "source": [
    "## (1) Overall Summary of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bef51",
   "metadata": {},
   "source": [
    "**----**The given Employee dataset consist of 1200 rows. The features present in the data are 28 columns. The shape of the dataset is 1200x28. \n",
    "\n",
    "**----**The 28 features are classified into quantitative and qualitative where 19 features are quantitative (11 columns consists numeric data & 8 columns consists ordinal data) and 8 features are qualitative. EmpNumber consist alphanumerical data (distinct values) which doesn't play a role as a relevant feature for performance rating.\n",
    "\n",
    "**----**There are no null values or constant values present in the data.\n",
    "\n",
    "**----**The analysis of the project has gone through the stage of Univariate & Bivariate analysis,and analysis by each department to satisfy the project goal.\n",
    "\n",
    "**----**All the features doesn't follows normal distribution hence used IQR method for outlier analysis.\n",
    "\n",
    "**----**Since all other outliers except the outlier for the feature 'TrainingTimesLastYear' are within 5% limit we removed outlier of this feature only.\n",
    "\n",
    "**----**From Correlation we can get the important aspects of the data, Correlation between features and Performance Rating.Correlation is a statistical measure that expresses the extent to which two variables are linearly related.There are no highly correlated features present in our data\n",
    "\n",
    "**----**The dataset consists of Categorical data and Numerical data. The Target variable consist of ordinal data, so this is a classification problem.\n",
    "\n",
    "**----**The main technique used in the preprocessing data using the Mannual & Frequency encoding method to convert the string - categorical data into numerical data, because, Most of machine learning methods are based on numerical methods where strings are not supportive. The overall project was performed and achieved the goals by using the machine learning model and visualization techniques.\n",
    "\n",
    "**----**The multiple machine learning model used in this project is Decision Tree, Support vector classifier, Random forest classifier, Logistic Regression, K Nearest Neighbour, Gradient Boosting & Artifical neural network. from above all models Random Forest Classifier predicts higher accuracy 92%.\n",
    "\n",
    "**----**One of the important goal of this project is to find the important feature affecting the performance rating. The important features were predicted using the machine learning model feature importance technique.The top 3 Important Factors effecting employee performance are: 'EmpLastSalaryHikePercent', 'EmpEnvironmentSatisfaction', 'YearsSinceLastPromotion'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008ea25",
   "metadata": {},
   "source": [
    "## (2) Requirement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa3b25",
   "metadata": {},
   "source": [
    "**----**The data was given from the IABAC for this project where the collected source is IABAC™. \n",
    " \n",
    "**----**The data is based on INX Future Inc, (referred as INX ). It is one of the leading data analytics and automation solutions provider with over 15 years of global business presence. INX is consistently rated as top 20 best employers past 5 years. The data is not from the real organization. \n",
    " \n",
    " \n",
    "**----**INX human resource policies are considered as employee friendly and widely perceived as best practices in the industry. Recent years, the employee performance indexes are not healthy and this is becoming a growing concerns among the top management. There has been increased escalations on service delivery and client satisfaction levels came down by 8 percentage points.\n",
    " \n",
    "**----**The CEO Mr. Brain, decided to initiate a data science project, which analyzes the current employee data and find the core underlying causes of the performance issues. He also expects a clear indicators of non-performing employees, so that any penalization of non-performing employee, if required, may not significantly affect other employee morals.             \n",
    " \n",
    "**----**The following insights are expected from this project:\n",
    " - Department wise performances.\n",
    " - Top 3 Important Factors effecting employee performance.\n",
    " - A trained model which can predict the employee performance based on factors as inputs.\n",
    " - Recommendations to improve the employee performance based on insights from analysis.\n",
    " \n",
    "**----**The whole project was done in Jupiter notebook with python platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976810b",
   "metadata": {},
   "source": [
    "## (3) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530296ef",
   "metadata": {},
   "source": [
    "- Data were analyzed by describing the features present in the data. the features play the bigger part in the analysis. The features tell the relation between the dependent and independent variables. Pandas also help to describe the data.\n",
    "- The data is supervised and categorical. The predictor variables are ordinal and a few among them are nominal. The target variable 'Performance Rating' is ordinal. \n",
    "- To analyze the data, various data processing techniques like Label Encoding and Standardization is used. Correlation Coeffecient is used to interpret the relationship between variables. The most important features selected are Environment Satisfaction, Last Salary Hike Percent,Years Since Last Promotion. \n",
    "- For training the data and predicting the target, algorithms used are Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, K-Nearest Neighbor, GradientBoosting Classifier and Artificial Neural Network. \n",
    "- A separate analysis of Department wise Performance is carried out.\n",
    "- The data present in the dataset are divided into numerical and categorical data.\n",
    "   \n",
    "   \n",
    "   Categorical Features¶\n",
    "- EmpNumber\n",
    "- Gender\n",
    "- EducationBackground\n",
    "- MaritalStatus\n",
    "- EmpDepartment\n",
    "- EmpJobRole\n",
    "- BusinessTravelFrequency\n",
    "- OverTime\n",
    "- Attrition\n",
    "\n",
    "\n",
    "  Numerical Features¶\n",
    "    \n",
    "- Age \n",
    "- DistanceFromHome\n",
    "- EmpEducationLevel\n",
    "- EmpEnvironmentSatisfaction\n",
    "- EmpHourlyRate\n",
    "- EmpJobInvolvement\n",
    "- EmpJobLevel\n",
    "- EmpJobSatisfaction\n",
    "- NumCompaniesWorked\n",
    "- EmpLastSalaryHikePercent\n",
    "- EmpRelationshipSatisfaction\n",
    "- TotalWorkExperienceInYears\n",
    "- TrainingTimesLastYear\n",
    "- EmpWorkLifeBalance\n",
    "- ExperienceYearsAtThisCompany\n",
    "- ExperienceYearsInCurrentRole\n",
    "- YearsSinceLastPromotion\n",
    "- YearsWithCurrManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d6595",
   "metadata": {},
   "source": [
    "### Univariate & Bivariate Analysis\n",
    "\n",
    "Library Used: Matplotlib & Seaborn\n",
    "Plots Used: CountPlot\n",
    "Tip: All Observation or insights written below the plots\n",
    "\n",
    "- Univariate Analysis: In univariate analysis we get the unique labels of categorical features, as well as get the range & density of numbers.\n",
    "\n",
    "- Bivariate Analysis: In bivariate analysis we check the feature relationship with target veriable.\n",
    "\n",
    "\n",
    "### Explotary Data Analysis\n",
    "\n",
    "##### Basic Check & Statistical Measures\n",
    "\n",
    "Their is no constant column present in Numerical as well as categoriacl data.\n",
    "Distribution of Continuous Features:\n",
    "In general, one of the first few steps in exploring the data would be to have a rough idea of how the features are distributed with one another. To do so, we shall invoke the familiar distplot function from the Seaborn plotting library. The distribution has been done by both numerical features. it will show the overall idea about the density and majority of data present in a different level.\n",
    "\n",
    "- The age distribution is starting from 18 to 60 where the most of the employees are laying between 30 to 40 age count\n",
    "- Employees are worked in the multiple companies up to 8 companies where most of the employees worked up to 2 companies before getting to work here.\n",
    "- The hourly rate range is 65 to 95 for majority employees work in this company.\n",
    "- In General, Most of Employees work up to 5 years in this company. Most of the employees get 11% to 15% of salary hike in this company.\n",
    "\n",
    "### Data Pre-Processing\n",
    "1.Check Missing Value: Their is no missing value in data\n",
    "\n",
    "2.Categorical Data Conversion: Handling categorical data with the help of frequency and mannual encoding, because feature is contain lot's of labels\n",
    "\n",
    "- Mannual Encoding: Mannual encoding is a best techinque to handel categorical feature with the help of map function, map the labels based on frequency.\n",
    "\n",
    "- Frequency Encoding: Frequency encoding is an encoding technique to transform an original categorical variable to a numerical variable by considering the frequency distribution of the data getting value counts.\n",
    "\n",
    "3.Outlier Handling Some features are contain outliers so we are impute this outlier with the help of IQR because in all features data is not normally distributed\n",
    "\n",
    "4.Scaling The Data: scaling the data with the help of Standard scalar\n",
    "\n",
    "- Standard Scaling: Standardization is the process of scaling the feature, it assumes the feature follow normal distribution and scale the feature between mean and standard deviation, here mean is 0 and standard deviation is always 1.\n",
    "\n",
    "6.Feature Selection\n",
    "1.Drop unique and constant feature: Dropping employee number because this is a constant column.\n",
    "\n",
    "2.Checking Correlation: Checking correlation with the help of heat map, There are no highly correlated feature present.\n",
    "\n",
    "- Heatmap: A heatmap is a graphical representation of data that uses a system of color-coding to represent different values.\n",
    "3.Check Duplicates: In this data Their is no dupicates is present.\n",
    "\n",
    "4.PCA: Use pca to reduce the dimension of data.Here after using pca accuracy is reducing for models.\n",
    "- Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset.\n",
    "\n",
    "### Machine learning Model Creation & Evaluation\n",
    "1.Define Dependant and Independant Features:\n",
    "\n",
    "2.Splitting Training And Testing Data: 80% data use for training & 20% data used for testing\n",
    "\n",
    "Algorithm:\n",
    "AIM: Create a sweet spot model (Low bias, Low variance)\n",
    "\n",
    "We have used models such as Decision tree, Randomforest, Gradientboosting (All with and without hyperparameter tuning),and after scaling i have used Logistic Regression, Support Vector Machine, Bagging of SVM, KNN models, Also fitted Logistic regression and randomforest models using PCA and the best training model to predict the employee performance based on factors as inputs is RandomForest Classifier with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72e573",
   "metadata": {},
   "source": [
    "## (4) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366b749",
   "metadata": {},
   "source": [
    "\n",
    "- ***1)***Department wise performance- Most employees are from  development department with around 300 employees having performance rate of 3,around 10 employees are having performance rate of 2 and around 40 employees are having the rate of 4.The Department having lowest performance rate is Data Science with 25 emploees having performance rate of 3, around 2 employees are having performance rate of 2 and around 3 employees are having the rate of 4.\n",
    "\n",
    "- ***2)***The top 3 Important Factors effecting employee performance are:'EmpLastSalaryHikePercent', 'EmpEnvironmentSatisfaction', 'YearsSinceLastPromotion'\n",
    "\n",
    "- ***3)***I have used models such as Decision tree, Randomforest, Gradientboosting (All with and without hyperparameter tuning),and after scaling i have used Logistic Regression,  Support Vector Machine, Bagging of SVM, KNN models, Also fitted Logistic regression and randomforest models using PCA and the best training model to predict the employee performance based on factors as inputs is RandomForest Classifier with hyperparameter tuning.\n",
    "\n",
    "- ***4)***The steps the company has to take to improve the employee performance based on insights from analysis are:\n",
    "\n",
    "   ***(a)***Employees with a hike percent of 11-14 are giving higher performnace than the employees with high hike Thus,            company would need to make hike percent to 11-14 percent for most employees.\n",
    "\n",
    "   ***(b)***Employees having high environment satisfaction are giving high performance Thus, the company would need to consider    most employees needs and take steps that will make every employees satisfied with their working environment.\n",
    "\n",
    "   ***(c)***Employees that got promotion within 0-2 yeears are giving high performance Thus, company has to take steps to give      promotions to most employee atleast within these year gaps atleast.\n",
    "\n",
    "   Since the important features effecting the performance of employees     are'EmpLastSalaryHikePercent','EmpEnvironmentSatisfaction', 'YearsSinceLastPromotion'\n",
    "- Insight:\n",
    "-    It was observed that the maximum accuracy was obtained when we used Random Forest with GridSearchCV which was 92%. GradientBoosting Classifier also yielded an accuracy of 92%. \n",
    "-    The important features are Environment Satisfaction, Last Salary Hike Percent & Years Since Last Promotion. This means that if these factors increases, Performance Rating will increase. On the other hand, the features with least importance are Attrition,Training time last year,Employment relationship satisfaction, overtime,etc. This means that if these factors increases, Performance Rating will go down.\n",
    "-    We can conclude that the company should provide a better environment as it increases the performance drastically. The company should increase the salary of the employee from time to time and help them maintain a worklife balance. On the other hand, shuffling the manager from time to time will also affect performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e71beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
